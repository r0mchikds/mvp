{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7cd3b9",
   "metadata": {},
   "source": [
    "### Назначение ноутбука\n",
    "\n",
    "В этом ноутбуке реализовано обучение и оценка качества моделей рекомендаций на основе различных подходов:\n",
    "\n",
    "1. **Baseline 0** — рекомендации по популярности (простая эвристика).\n",
    "2. **Baseline 1** — модель CatBoost с текстовыми TF-IDF признаками.\n",
    "3. **MatchingMLP** — нейросетевая модель, обучающаяся на конкатенированных векторах пользователя и товара.\n",
    "\n",
    "Данные использованы после предварительной разметки данных на основе поставленной пользователем оценки товару, а именно 1 при оценке >= 4, и 0 - в иных случаях (rating-based sampling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac20fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "disable_tqdm = os.getenv(\"TQDM_DISABLE\", \"0\") == \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e3ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b39184",
   "metadata": {},
   "source": [
    "### Импорт данных и предобработка\n",
    "\n",
    "Загружаются:\n",
    "\n",
    "* `df_train` — обучающая выборка с метками взаимодействия;\n",
    "* `df_test` — лог реальных взаимодействий пользователей;\n",
    "* `df_meta` — признаки товаров, включая эмбеддинги CLIP и TF-IDF, числовые и категориальные признаки.\n",
    "\n",
    "Для использования в разных моделях далее будут созданы отдельные версии датафрейма:\n",
    "\n",
    "* `df_meta_tfidf` — без CLIP-признаков (только TF-IDF);\n",
    "* `df_meta_clip` — без TF-IDF (только CLIP + табличные признаки).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c40911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем тестовый датасет и обучающую выборку (rating-based sampling)\n",
    "df_test = pd.read_csv(\"data/df_test_ground_truth_rating_based.csv\")\n",
    "df_train = pd.read_csv(\n",
    "    \"data/df_train_baseline_0_rating_based.csv\",\n",
    "    na_values=[\"\"],  # исключаем \"Unknown\"\n",
    "    keep_default_na=False\n",
    ")\n",
    "\n",
    "# Загружаем мета-информацию с признаками TF-IDF и CLIP\n",
    "df_meta = pd.read_csv(\n",
    "    \"data/amazon_meta_clean.csv\",\n",
    "    na_values=[\"\"],  # исключаем \"Unknown\"\n",
    "    keep_default_na=False\n",
    ")\n",
    "# Приводим название колонки к общему формату\n",
    "df_meta.rename(columns={\"asin\": \"item_id\"}, inplace=True)\n",
    "# Удаляем исходные текстовые и визуальные поля, т.к. они не используются напрямую\n",
    "df_meta.drop(columns=[\"text_full\", \"image_main\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ffb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для TF-IDF модели: удалим CLIP-фичи\n",
    "tfidf_cols = [col for col in df_meta.columns if col.startswith(\"clip_text_\") or col.startswith(\"clip_img_\")]\n",
    "df_meta_tfidf = df_meta.drop(columns=tfidf_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cb507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для CLIP модели: удалим TF-IDF-фичи\n",
    "clip_cols = [col for col in df_meta.columns if col.startswith(\"tfidf_\")]\n",
    "df_meta_clip = df_meta.drop(columns=clip_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82522cc",
   "metadata": {},
   "source": [
    "## Baseline 0: Popularity-Based Recommender\n",
    "\n",
    "Наиболее простая стратегия: каждому пользователю рекомендуются самые популярные товары (наиболее часто встречающиеся среди положительных взаимодействий в трейне).\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "- Очень быстро считается\n",
    "- Не требует персонализации или фичей\n",
    "- Может служить нижней границей качества модели\n",
    "\n",
    "### Ограничения\n",
    "\n",
    "- Не учитывает интересы конкретного пользователя\n",
    "- Невозможно адаптироваться под редкие/новые товары\n",
    "- Игнорирует временную динамику и мультимодальные признаки\n",
    "\n",
    "---\n",
    "\n",
    "Мы сравним этот бейзлайн с более продвинутыми моделями позже: CatBoost, CLIP + текст/изображения и т.д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766392f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Самые популярные товары (по кол-ву позитивных взаимодействий)\n",
    "popular_items = (\n",
    "    df_train[df_train[\"label\"] == 1][\"item_id\"]\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .index\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de4f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание: каждому пользователю - один и тот же топ-10\n",
    "predictions = {user: popular_items for user in df_test[\"user_id\"].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d64dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "# Сопоставляем каждому пользователю множество товаров,\n",
    "# с которыми он взаимодействовал в тесте\n",
    "ground_truth = df_test.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec19ef8",
   "metadata": {},
   "source": [
    "## Оценка качества: Precision@K и MAP@K\n",
    "\n",
    "После генерации рекомендаций для каждого пользователя (`predictions`) мы хотим оценить качество модели с помощью стандартных метрик:\n",
    "\n",
    "### Precision@K\n",
    "- Показывает долю релевантных товаров (из ground truth), попавших в top-K рекомендаций\n",
    "- Рассчитывается для каждого пользователя, затем усредняется\n",
    "\n",
    "### MAP@K (Mean Average Precision)\n",
    "- Учитывает не только попадание, но и **позицию** релевантного товара в списке\n",
    "- Чем выше релевантные товары в списке — тем лучше\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fee1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция Precision@K — средняя доля релевантных товаров среди top-K рекомендаций\n",
    "def precision_at_k(preds, ground_truth, k=10):\n",
    "    scores = []\n",
    "    for user, pred_items in preds.items():\n",
    "        if user not in ground_truth:\n",
    "            continue\n",
    "        gt_items = ground_truth[user]\n",
    "        hits = sum([1 for item in pred_items[:k] if item in gt_items])\n",
    "        scores.append(hits / k)\n",
    "    return round(np.mean(scores), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd227cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция average precision для одного пользователя\n",
    "def apk(pred, actual, k=10):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred = pred[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(pred):\n",
    "        if p in actual and p not in pred[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a1c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Средняя average precision по всем пользователям\n",
    "def map_at_k(preds, ground_truth, k=10):\n",
    "    return round(\n",
    "        np.mean([\n",
    "            apk(preds[u], ground_truth[u], k)\n",
    "            for u in preds if u in ground_truth\n",
    "        ]),\n",
    "        4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64356a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0015\n",
      "MAP@10: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Оцениваем качество рекомендаций по популярности (Baseline 0)\n",
    "print(\"Precision@10:\", precision_at_k(predictions, ground_truth, k=10))\n",
    "print(\"MAP@10:\", map_at_k(predictions, ground_truth, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a3ccd",
   "metadata": {},
   "source": [
    "## Baseline 1: TF-IDF + PCA + CatBoost\n",
    "\n",
    "Следующий бейзлайн использует простую, но более информативную стратегию: **машинное обучение на основе текстовых признаков**.\n",
    "\n",
    "### Идея\n",
    "\n",
    "- Используем текстовое описание товара (`text_full`) и превращаем его в числовой вектор с помощью **TF-IDF**.\n",
    "- Чтобы избежать переобучения и снизить размерность — применяем **PCA**.\n",
    "- Модель **CatBoost** обучается на этих эмбеддингах, предсказывая вероятность релевантности товара для пользователя.\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "- Учитывает смысловое содержание описания товара.\n",
    "- Обеспечивает некоторую степень персонализации.\n",
    "- Прост в реализации и достаточно эффективен на структурированных данных.\n",
    "\n",
    "### Ограничения\n",
    "\n",
    "- Не использует изображение (визуальную составляющую товара).\n",
    "- Не моделирует поведение пользователя напрямую (работает на item-фичах).\n",
    "- Возможны потери информации при снижении размерности через PCA.\n",
    "\n",
    "---\n",
    "\n",
    "Этот бейзлайн служит первым шагом от простых эвристик к ML-подходам. Далее мы добавим **визуальные эмбеддинги CLIP**, чтобы построить мультимодальную модель.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c1f9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d6ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет для baseline-модели на TF-IDF фичах\n",
    "df_train = pd.read_csv(\n",
    "    \"data/df_train_baseline_1_rating_based.csv\",\n",
    "    na_values=[\"\"],  # исключаем \"Unknown\"\n",
    "    keep_default_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01cccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Списки признаков: tf-idf фичи, числовые признаки, категориальные признаки\n",
    "tfidf_cols = [col for col in df_train.columns if col.startswith(\"tfidf_\")]\n",
    "meta_features = [\n",
    "    \"title_len\",\n",
    "    \"title_has_digit\",\n",
    "    \"description_text_len\",\n",
    "    \"is_top20_brand\",\n",
    "    \"has_price\",\n",
    "    \"is_top9_category_main\"\n",
    "]\n",
    "features = (\n",
    "    tfidf_cols +\n",
    "    meta_features +\n",
    "    [\"brand\", \"category_main\", \"price_clean\"]\n",
    ")\n",
    "cat_features = [\"brand\", \"category_main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d399c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пул для catboost\n",
    "train_pool = Pool(df_train[features], label=df_train[\"label\"], cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0b6f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем и обучим модель\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    bootstrap_type=\"Bayesian\",\n",
    "    eval_metric=\"AUC\",\n",
    "    random_seed=42,\n",
    "    verbose=200,\n",
    "    task_type=\"GPU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f224bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 164ms\tremaining: 2m 43s\n",
      "200:\ttotal: 27.2s\tremaining: 1m 48s\n",
      "400:\ttotal: 55.4s\tremaining: 1m 22s\n",
      "600:\ttotal: 1m 24s\tremaining: 55.9s\n",
      "800:\ttotal: 1m 53s\tremaining: 28.2s\n",
      "999:\ttotal: 2m 23s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1b7925a11c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d75201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказываем вероятности для всех товаров из df_meta_tfidf\n",
    "# Сортируем по вероятности и выбираем top-10 и top-50 для каждого пользователя\n",
    "user_ids = df_test[\"user_id\"].unique()\n",
    "predictions = {}\n",
    "# predictions_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f683ec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85083/85083 [17:44:04<00:00,  1.33it/s]  \n"
     ]
    }
   ],
   "source": [
    "for user in tqdm(user_ids, file=sys.stdout):\n",
    "    df_user = df_meta_tfidf.copy()\n",
    "    df_user[\"user_id\"] = user\n",
    "\n",
    "    pool = Pool(df_user[features], cat_features=cat_features)\n",
    "    df_user[\"pred\"] = model.predict_proba(pool)[:, 1]\n",
    "\n",
    "    # Оригинальный топ-10 (для грязных метрик)\n",
    "    predictions[user] = (\n",
    "        df_user.sort_values(\"pred\", ascending=False)[\"item_id\"]\n",
    "        .head(10)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # # Расширенный пул топ-50\n",
    "    # predictions_pool[user] = (\n",
    "    #     df_user.sort_values(\"pred\", ascending=False)[\"item_id\"]\n",
    "    #     .head(50)\n",
    "    #     .tolist()\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a0bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_test.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b447008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1 (по всем товарам)\n",
      "Precision@10: 0.0001\n",
      "MAP@10: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Оцениваем Precision@10 и MAP@10 для модели на TF-IDF по всем товарам\n",
    "print(\"Baseline 1 (по всем товарам)\")\n",
    "print(\"Precision@10:\", precision_at_k(predictions, ground_truth, k=10))\n",
    "print(\"MAP@10:\", map_at_k(predictions, ground_truth, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e90d7",
   "metadata": {},
   "source": [
    "### Matching MLP — нейросетевая модель\n",
    "\n",
    "Формируется обучающая выборка: конкатенируются векторы пользователя (среднее по вектору его положительных товаров) и вектор текущего товара.\n",
    "\n",
    "Нейросеть `MatchingMLP` обучается отличать положительные пары от отрицательных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da753057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем тренировочную выборку с CLIP-эмбеддингами (текст + изображение)\n",
    "df_train = pd.read_csv(\n",
    "    \"data/df_train_CLIP_rating_based.csv\",\n",
    "    na_values=[\"\"],  # исключаем \"Unknown\"\n",
    "    keep_default_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c40f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем CLIP-эмбеддинги (текст + изображение)\n",
    "item_vector_cols = [col for col in df_train.columns if col.startswith(\"clip_text_\") or col.startswith(\"clip_img_\")]\n",
    "user_ids = df_train[\"user_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1209c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем дополнительные табличные признаки, которые добавим к CLIP-вектору\n",
    "extra_vec_cols = [\n",
    "    \"title_len\",\n",
    "    \"description_text_len\",\n",
    "    \"is_top20_brand\",\n",
    "    \"has_price\",\n",
    "    \"is_top9_category_main\",\n",
    "    \"price_clean\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf90b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Избавляемся от дубликатов товаров и нормализуем числовые признаки\n",
    "df_item_extra = df_train.drop_duplicates(\"item_id\")[[\"item_id\"] + extra_vec_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08219fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_extra[\"price_clean\"] = df_item_extra[\"price_clean\"].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24d2e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_item_extra[[\"title_len\", \"description_text_len\", \"price_clean\"]] = scaler.fit_transform(\n",
    "    df_item_extra[[\"title_len\", \"description_text_len\", \"price_clean\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeed9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводим в индекс item_id для удобного объединения\n",
    "df_item_extra = df_item_extra.set_index(\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2c12fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем CLIP-эмбеддинги и объединяем с табличными признаками\n",
    "clip_vectors  = df_train.drop_duplicates(\"item_id\")[[\"item_id\"] + item_vector_cols].set_index(\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54b7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_vectors = pd.concat([clip_vectors, df_item_extra], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8e112de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_vectors shape: (67706, 6)\n",
      "clip_vectors shape: (67706, 200)\n",
      "item_vectors shape: (67706, 206)\n"
     ]
    }
   ],
   "source": [
    "print(\"extra_vectors shape:\", df_item_extra.shape)\n",
    "print(\"clip_vectors shape:\", clip_vectors.shape)\n",
    "print(\"item_vectors shape:\", item_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e31e330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем вектор интересов пользователя как среднее по позитивным item-векторам\n",
    "user_vectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b366de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User vector aggregation: 100%|██████████| 551853/551853 [02:51<00:00, 3219.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for user_id, group in tqdm(df_train[df_train[\"label\"] == 1].groupby(\"user_id\"), desc=\"User vector aggregation\", file=sys.stdout):\n",
    "    item_ids = group[\"item_id\"].values\n",
    "    vectors = item_vectors.loc[item_ids].values\n",
    "    user_vectors[user_id] = np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f7b5fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим датасет: конкатенируем векторы пользователя и товара\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d2d6bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training pairs: 100%|██████████| 1635089/1635089 [02:59<00:00, 9090.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(df_train.itertuples(), total=len(df_train), desc=\"Building training pairs\", file=sys.stdout):\n",
    "    item_id = row.item_id\n",
    "    user_id = row.user_id\n",
    "    label = row.label\n",
    "\n",
    "    if user_id not in user_vectors or item_id not in item_vectors.index:\n",
    "        continue\n",
    "\n",
    "    user_vec = user_vectors[user_id]\n",
    "    item_vec = item_vectors.loc[item_id].values\n",
    "\n",
    "    concat_vec = np.concatenate([user_vec, item_vec])\n",
    "    X.append(concat_vec)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8259c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1542533, 412)\n",
      "y shape: (1542533,)\n"
     ]
    }
   ],
   "source": [
    "# Преобразуем списки в массивы для дальнейшей подачи в PyTorch\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35939747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем PyTorch и определяем, использовать ли GPU\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6aa2d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eabaecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем Dataset\n",
    "class MatchingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66106927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём датасет и делим его на обучающую и валидационную части\n",
    "full_dataset = MatchingDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c2c5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1be2f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оборачиваем датасеты в DataLoader для удобной подачи батчами\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73f78f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем MLP модель\n",
    "class MatchingMLP(nn.Module):\n",
    "    def __init__(self, input_dim=400):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf3e88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём экземпляр модели\n",
    "model = MatchingMLP(input_dim=X.shape[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3987e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем бинарную кросс-энтропию в качестве функции потерь\n",
    "# и Adam как оптимизатор с learning rate 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08263834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Обучаем модель в течение 10 эпох, отслеживая loss и accuracy на валидации\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "204b833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 678/678 [00:12<00:00, 52.19it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 48.70it/s]\n",
      "Epoch 1/10 | Train Loss: 0.2840 | Val Loss: 0.2185 | Val Accuracy: 0.9202\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 45.55it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 49.98it/s]\n",
      "Epoch 2/10 | Train Loss: 0.2074 | Val Loss: 0.1671 | Val Accuracy: 0.9403\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 46.89it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 50.20it/s]\n",
      "Epoch 3/10 | Train Loss: 0.1558 | Val Loss: 0.1291 | Val Accuracy: 0.9542\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 47.07it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 55.31it/s]\n",
      "Epoch 4/10 | Train Loss: 0.1278 | Val Loss: 0.1104 | Val Accuracy: 0.9611\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 46.31it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 49.98it/s]\n",
      "Epoch 5/10 | Train Loss: 0.1127 | Val Loss: 0.1015 | Val Accuracy: 0.9643\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 46.94it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 50.42it/s]\n",
      "Epoch 6/10 | Train Loss: 0.1036 | Val Loss: 0.0990 | Val Accuracy: 0.9655\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 46.85it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 54.27it/s]\n",
      "Epoch 7/10 | Train Loss: 0.0971 | Val Loss: 0.0918 | Val Accuracy: 0.9682\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 47.33it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 54.78it/s]\n",
      "Epoch 8/10 | Train Loss: 0.0922 | Val Loss: 0.0917 | Val Accuracy: 0.9683\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 46.05it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 54.78it/s]\n",
      "Epoch 9/10 | Train Loss: 0.0886 | Val Loss: 0.0866 | Val Accuracy: 0.9702\n",
      "Training: 100%|██████████| 678/678 [00:14<00:00, 46.51it/s]\n",
      "Validating: 100%|██████████| 76/76 [00:01<00:00, 54.78it/s]\n",
      "Epoch 10/10 | Train Loss: 0.0846 | Val Loss: 0.0854 | Val Accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=\"Training\", file=sys.stdout):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).view(-1, 1)\n",
    "\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for X_batch, y_batch in tqdm(val_loader, desc=\"Validating\", file=sys.stdout):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).view(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "        val_losses.append(loss.item())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Бинаризуем предсказания, считаем accuracy и выводим метрики за эпоху\n",
    "    all_preds_bin = (np.array(all_preds) >= 0.5).astype(int)\n",
    "    val_acc = accuracy_score(all_targets, all_preds_bin)\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"Train Loss: {np.mean(train_losses):.4f} | \"\n",
    "        f\"Val Loss: {np.mean(val_losses):.4f} | \"\n",
    "        f\"Val Accuracy: {val_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d40fbab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (138023, 5), unique users: 85083\n"
     ]
    }
   ],
   "source": [
    "# Проверяем размер и разнообразие тестового сета перед инференсом\n",
    "print(f\"Test shape: {df_test.shape}, unique users: {df_test['user_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "520ba600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём словарь ground truth: реальные товары, \n",
    "# с которыми взаимодействовал пользователь\n",
    "ground_truth = df_test.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd6c02fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatchingMLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=412, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переводим модель в режим инференса\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4dca6304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67706, 206)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготавливаем список кандидатов и их эмбеддинги для инференса\n",
    "candidate_items = item_vectors.index.tolist()\n",
    "candidate_vectors = item_vectors.values\n",
    "candidate_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "978cc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для каждого пользователя генерируем top-N рекомендаций\n",
    "# Сортировка кандидатов по вероятности релевантности (score)\n",
    "top_k = 10\n",
    "predictions = {}\n",
    "# predictions_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eeb2a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating top-N: 100%|██████████| 85083/85083 [5:53:11<00:00,  4.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "for user_id in tqdm(ground_truth.keys(), desc=\"Generating top-N\", file=sys.stdout):\n",
    "    if user_id not in user_vectors:\n",
    "        continue\n",
    "\n",
    "    user_vec = user_vectors[user_id]\n",
    "    user_vec_batch = np.tile(user_vec, (candidate_vectors.shape[0], 1))\n",
    "    concat = np.hstack([user_vec_batch, candidate_vectors])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(torch.tensor(concat, dtype=torch.float32).to(device)).cpu().numpy().flatten()\n",
    "\n",
    "    sorted_items = np.array(candidate_items)[np.argsort(scores)[::-1]]\n",
    "    predictions[user_id] = sorted_items[:10].tolist()\n",
    "    # predictions_pool[user_id] = sorted_items[:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2be5d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching MLP (top-10)\n",
      "Precision@10: 0.0015\n",
      "MAP@10: 0.0047\n"
     ]
    }
   ],
   "source": [
    "# Считаем метрики на всём списке рекомендаций\n",
    "print(\"Matching MLP (top-10)\")\n",
    "print(\"Precision@10:\", precision_at_k(predictions, ground_truth))\n",
    "print(\"MAP@10:\", map_at_k(predictions, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a08a29",
   "metadata": {},
   "source": [
    "### Сравнение моделей по метрикам качества (rating-based, temporal split)\n",
    "\n",
    "Все метрики рассчитаны по **всему пулу товаров**, без фильтрации уже просмотренных пользователем товаров.\n",
    "\n",
    "| Модель                  | Precision@10 | MAP@10  |\n",
    "|-------------------------|--------------|---------|\n",
    "| **Baseline 0 (Popular)**| **0.0015**   | 0.002   |\n",
    "| **Baseline 1 (TF-IDF)** | 0.0001       | 0.0001  |\n",
    "| **Matching MLP (CLIP)** | **0.0015**       | **0.0047** |\n",
    "\n",
    "---\n",
    "\n",
    "### Выводы:\n",
    "\n",
    "* **Baseline 0 (popular)** показал наивысший **Precision@10**, несмотря на простоту. Это подчёркивает, что в задачах без персонализации популярные товары могут быть сильным ориентиром.\n",
    "* **Matching MLP** пока не превосходит популярность по точности, но даёт **более высокое MAP@10**, что указывает на **лучшее ранжирование релевантных товаров** в пределах топа.\n",
    "* **Baseline 1 (TF-IDF + CatBoost)** продемонстрировал крайне низкие результаты — как по точности, так и по позиционному качеству. Это подтверждает ограниченность TF-IDF признаков в условиях rating-based и большого пула товаров.\n",
    "* Полученные значения задают **честный baseline** для дальнейшего улучшения модели:\n",
    "  - добавления **user-based признаков**;\n",
    "  - замены head'а с MLP на **dot-product**;\n",
    "  - оптимизации признаков и гиперпараметров.\n",
    "\n",
    "---\n",
    "\n",
    "Модель работает честно, метрики реалистичны, и мы готовы к улучшениям.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
