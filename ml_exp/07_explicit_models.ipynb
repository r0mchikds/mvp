{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40548f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit Matching Models for Laptop Evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import faiss\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0866a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "train_path = \"data/df_train_CLIP_rating_based.csv\"\n",
    "test_path = \"data/df_test_ground_truth_rating_based.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6d07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_path)\n",
    "df_train = pd.read_csv(\n",
    "    train_path,\n",
    "    na_values=[\"\"],  # исключаем \"Unknown\"\n",
    "    keep_default_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff56801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение ground_truth\n",
    "ground_truth = (\n",
    "    df_test[df_test[\"label\"] == 1]\n",
    "    .groupby(\"user_id\")[\"item_id\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf43999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@K and MAP@K\n",
    "def precision_at_k(preds, ground_truth, k=10):\n",
    "    scores = []\n",
    "    for user, pred_items in preds.items():\n",
    "        if user not in ground_truth:\n",
    "            continue\n",
    "        gt_items = ground_truth[user]\n",
    "        hits = sum([1 for item in pred_items[:k] if item in gt_items])\n",
    "        scores.append(hits / k)\n",
    "    return round(np.mean(scores), 4)\n",
    "\n",
    "def apk(pred, actual, k=10):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    pred = pred[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(pred):\n",
    "        if p in actual and p not in pred[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def map_at_k(preds, ground_truth, k=10):\n",
    "    return round(\n",
    "        np.mean([apk(preds[u], ground_truth[u], k) for u in preds if u in ground_truth]), 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db3459",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa0ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN over CLIP Embeddings \n",
    "def run_clip_knn(df_train, df_test, k):\n",
    "    emb_cols = [c for c in df_train.columns if c.startswith(\"clip_text_\") or c.startswith(\"clip_img_\")]\n",
    "    df_items = df_train.drop_duplicates(\"item_id\")[[\"item_id\"] + emb_cols].dropna().set_index(\"item_id\")\n",
    "    model_knn = NearestNeighbors(n_neighbors=k+1, metric=\"cosine\")\n",
    "    model_knn.fit(df_items.values)\n",
    "    preds = {}\n",
    "    for uid, group in df_test.groupby(\"user_id\"):\n",
    "        items = [i for i in group[\"item_id\"].unique() if i in df_items.index]\n",
    "        if not items:\n",
    "            continue\n",
    "        sim = []\n",
    "        for i in items:\n",
    "            idx = df_items.index.get_loc(i)\n",
    "            _, inds = model_knn.kneighbors([df_items.iloc[idx].values])\n",
    "            sim.extend(df_items.iloc[inds[0][1:]].index.tolist())\n",
    "        preds[uid] = sim[:10]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d206dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS  over CLIP Embeddings\n",
    "def run_clip_faiss(df_train, df_test, top_k=10):\n",
    "    emb_cols = [c for c in df_train.columns if c.startswith(\"clip_text_\") or c.startswith(\"clip_img_\")]\n",
    "    df_items = df_train.drop_duplicates(\"item_id\")[[\"item_id\"] + emb_cols].dropna().set_index(\"item_id\")\n",
    "    \n",
    "    # Индексация через FAISS\n",
    "    index = faiss.IndexFlatIP(len(emb_cols))  # dot-product\n",
    "    item_vectors = np.ascontiguousarray(df_items.values.astype(\"float32\"))\n",
    "    faiss.normalize_L2(item_vectors)\n",
    "    index.add(item_vectors)\n",
    "    item_ids = df_items.index.tolist()\n",
    "\n",
    "    preds = {}\n",
    "    for uid, group in df_test.groupby(\"user_id\"):\n",
    "        user_items = [i for i in group[\"item_id\"].unique() if i in df_items.index]\n",
    "        if not user_items:\n",
    "            continue\n",
    "\n",
    "        user_vec = np.ascontiguousarray(df_items.loc[user_items]\n",
    "                                        .values.astype(\"float32\"))\n",
    "        faiss.normalize_L2(user_vec)\n",
    "        user_mean = np.mean(user_vec, axis=0, keepdims=True)\n",
    "\n",
    "        _, indices = index.search(user_mean, top_k)\n",
    "        preds[uid] = [item_ids[i] for i in indices[0]]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb41aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "def evaluate_model(preds, ground_truth):\n",
    "    return {\n",
    "        \"precision\": precision_at_k(preds, ground_truth),\n",
    "        \"map\": map_at_k(preds, ground_truth)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae315ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5cffaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-KNN:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started iter for k = 5\n",
      "Preds done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-KNN:  33%|███▎      | 1/3 [2:34:12<5:08:25, 9252.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score calculated\n",
      "Finished iter for k = 5\n",
      "Started iter for k = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-KNN:  67%|██████▋   | 2/3 [5:07:03<2:33:24, 9204.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds done\n",
      "Score calculated\n",
      "Finished iter for k = 10\n",
      "Started iter for k = 20\n",
      "Preds done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-KNN: 100%|██████████| 3/3 [7:38:45<00:00, 9175.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score calculated\n",
      "Finished iter for k = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm([5, 10, 20], desc=\"CLIP-KNN\"):\n",
    "    print(f\"Started iter for k = {k}\")\n",
    "    preds = run_clip_knn(df_train, df_test, k)\n",
    "    print(\"Preds done\")\n",
    "    score = evaluate_model(preds, ground_truth)\n",
    "    print(\"Score calculated\")\n",
    "    results.append((\"CLIP_KNN\", k, score))\n",
    "    print(f\"Finished iter for k = {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677809d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-FAISS:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started iter for k = 5\n",
      "Preds done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-FAISS:  33%|███▎      | 1/3 [04:22<08:44, 262.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score calculated\n",
      "Finished iter for k = 5\n",
      "Started iter for k = 10\n",
      "Preds done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-FAISS:  67%|██████▋   | 2/3 [08:41<04:20, 260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score calculated\n",
      "Finished iter for k = 10\n",
      "Started iter for k = 20\n",
      "Preds done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP-FAISS: 100%|██████████| 3/3 [13:01<00:00, 260.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score calculated\n",
      "Finished iter for k = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- FAISS-based CLIP matching ---\n",
    "for k in tqdm([5, 10, 20], desc=\"CLIP-FAISS\"):\n",
    "    print(f\"Started iter for k = {k}\")\n",
    "    preds = run_clip_faiss(df_train, df_test, top_k=k)\n",
    "    print(\"Preds done\")\n",
    "    score = evaluate_model(preds, ground_truth)\n",
    "    print(\"Score calculated\")\n",
    "    results.append((\"CLIP_FAISS\", k, score))\n",
    "    print(f\"Finished iter for k = {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e35c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best results by model:\n"
     ]
    }
   ],
   "source": [
    "# Print best results\n",
    "print(\"\\nBest results by model:\")\n",
    "model_scores = defaultdict(list)\n",
    "for name, param, score in results:\n",
    "    model_scores[name].append((param, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a9693a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP_KNN: best_param=5, precision=0.0085, map=0.015\n",
      "CLIP_FAISS: best_param=10, precision=0.1358, map=0.9176\n"
     ]
    }
   ],
   "source": [
    "for model, entries in model_scores.items():\n",
    "    best = max(entries, key=lambda x: x[1][\"precision\"])\n",
    "    print(f\"{model}: best_param={best[0]}, precision={best[1]['precision']}, map={best[1]['map']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
